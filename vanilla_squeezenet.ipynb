{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import optimizers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "from squeezenet import SqueezeNet, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/home/ubuntu/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25600 images belonging to 256 classes.\n",
      "Found 5120 images belonging to 256 classes.\n"
     ]
    }
   ],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    data_format='channels_last',\n",
    "    preprocessing_function=preprocess_input\n",
    ")\n",
    "\n",
    "train_generator = data_generator.flow_from_directory(\n",
    "    data_dir + 'train', \n",
    "    target_size=(299, 299),\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "val_generator = data_generator.flow_from_directory(\n",
    "    data_dir + 'val', \n",
    "    target_size=(299, 299),\n",
    "    batch_size=128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SqueezeNet(weight_decay=1e-5, image_size=299)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizers.SGD(lr=1e-2, momentum=0.9, nesterov=True), \n",
    "    loss='categorical_crossentropy', metrics=['accuracy', 'top_k_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "400/400 [==============================] - 30s - loss: 2.3677 - acc: 0.4859 - top_k_categorical_accuracy: 0.7056 - val_loss: 1.7769 - val_acc: 0.5992 - val_top_k_categorical_accuracy: 0.8213\n",
      "Epoch 2/30\n",
      "400/400 [==============================] - 30s - loss: 1.1757 - acc: 0.7103 - top_k_categorical_accuracy: 0.8941 - val_loss: 1.6904 - val_acc: 0.6111 - val_top_k_categorical_accuracy: 0.8330\n",
      "Epoch 3/30\n",
      "400/400 [==============================] - 30s - loss: 0.8528 - acc: 0.7858 - top_k_categorical_accuracy: 0.9343 - val_loss: 1.6566 - val_acc: 0.6285 - val_top_k_categorical_accuracy: 0.8469\n",
      "Epoch 4/30\n",
      "400/400 [==============================] - 30s - loss: 0.6804 - acc: 0.8255 - top_k_categorical_accuracy: 0.9533 - val_loss: 1.6594 - val_acc: 0.6283 - val_top_k_categorical_accuracy: 0.8531\n",
      "Epoch 5/30\n",
      "400/400 [==============================] - 30s - loss: 0.5532 - acc: 0.8566 - top_k_categorical_accuracy: 0.9666 - val_loss: 1.7166 - val_acc: 0.6404 - val_top_k_categorical_accuracy: 0.8535\n",
      "Epoch 6/30\n",
      "400/400 [==============================] - 30s - loss: 0.4669 - acc: 0.8779 - top_k_categorical_accuracy: 0.9746 - val_loss: 1.6626 - val_acc: 0.6418 - val_top_k_categorical_accuracy: 0.8580\n",
      "Epoch 7/30\n",
      "400/400 [==============================] - 30s - loss: 0.3948 - acc: 0.8979 - top_k_categorical_accuracy: 0.9805 - val_loss: 1.7280 - val_acc: 0.6447 - val_top_k_categorical_accuracy: 0.8516\n",
      "Epoch 8/30\n",
      "400/400 [==============================] - 30s - loss: 0.3520 - acc: 0.9086 - top_k_categorical_accuracy: 0.9857 - val_loss: 1.7793 - val_acc: 0.6387 - val_top_k_categorical_accuracy: 0.8498\n",
      "Epoch 9/30\n",
      "400/400 [==============================] - 30s - loss: 0.2269 - acc: 0.9522 - top_k_categorical_accuracy: 0.9916 - val_loss: 1.6187 - val_acc: 0.6666 - val_top_k_categorical_accuracy: 0.8627- top_k_categorical_accu\n",
      "Epoch 10/30\n",
      "400/400 [==============================] - 30s - loss: 0.1939 - acc: 0.9652 - top_k_categorical_accuracy: 0.9929 - val_loss: 1.6135 - val_acc: 0.6670 - val_top_k_categorical_accuracy: 0.8650\n",
      "Epoch 11/30\n",
      "400/400 [==============================] - 30s - loss: 0.1880 - acc: 0.9677 - top_k_categorical_accuracy: 0.9937 - val_loss: 1.6192 - val_acc: 0.6629 - val_top_k_categorical_accuracy: 0.8617\n",
      "Epoch 12/30\n",
      "400/400 [==============================] - 30s - loss: 0.1806 - acc: 0.9695 - top_k_categorical_accuracy: 0.9943 - val_loss: 1.6309 - val_acc: 0.6602 - val_top_k_categorical_accuracy: 0.8643\n",
      "Epoch 13/30\n",
      "400/400 [==============================] - 30s - loss: 0.1718 - acc: 0.9727 - top_k_categorical_accuracy: 0.9944 - val_loss: 1.5822 - val_acc: 0.6719 - val_top_k_categorical_accuracy: 0.8650\n",
      "Epoch 14/30\n",
      "400/400 [==============================] - 30s - loss: 0.1701 - acc: 0.9742 - top_k_categorical_accuracy: 0.9943 - val_loss: 1.6067 - val_acc: 0.6689 - val_top_k_categorical_accuracy: 0.8641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fcb3b266048>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    train_generator, \n",
    "    steps_per_epoch=400, epochs=30, verbose=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_acc', patience=4, min_delta=0.01),\n",
    "        ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=2, epsilon=0.007)\n",
    "    ],\n",
    "    validation_data=val_generator, validation_steps=40, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5120 images belonging to 256 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator_no_shuffle = data_generator.flow_from_directory(\n",
    "    data_dir + 'val', \n",
    "    target_size=(299, 299),\n",
    "    batch_size=128, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6083734691143037, 0.66679687499999996, 0.86289062500000002]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate_generator(val_generator_no_shuffle, 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
